{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Buckets creation and notifications configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, if you've never used a Jupyter notebook before, here are a few information:\n",
    "- A notebook is an environment where you have *cells* that can display formatted text (like the present cell), or code (as you will see below).\n",
    "- Code cells contain Python code that can be run interactively. Thats means you can modify the code, as you will do in the first code cell, then run it. The code will not run on you computer or in the browser, but directly on the server you are connected to.\n",
    "- To run a code cell, just select it (click in the cell, or on the left side of it), and click the \"Run/Play\" button from the toolbar.\n",
    "- You can also press CTRL+Enter to run a cell, or Shift+Enter to run the cell and automatically select the following one.\n",
    "- You can navigate between cells with Up and Down arrows (on your keyboard or in the toolbar).\n",
    "\n",
    "With this (very) quick turorial, you should be able to run the different parts of this notebook. If you still have a doubt, here is a longer tutorial: https://www.codecademy.com/articles/how-to-use-jupyter-notebooks\n",
    "\n",
    "Now that you have the basics, please read the instructions and run ALL the cells in the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparatory steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ---==== REALLY IMPORTANT STEP! ====---\n",
    "### Please read carefully and make all the requested changes. There are 4 changes to make.\n",
    "## Parameters\n",
    "\n",
    "We will first start by setting some paramaters that will be used in this notebook. Replace the ones where indicated with the needed values, and run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warning!!!: For all those variables, don't remove the starting/ending quotes when doing copy/paste\n",
    "\n",
    "# Enter the main namespace name of this lab. It should be in the form xraylab-{number}, like xraylab-3\n",
    "namespace = 'xraylab-2'\n",
    "\n",
    "# Enter you bucket base name. That's the same one you have put in the config map. It should be identical as your namespace if you have followed the instructions.\n",
    "bucket_base_name = 'xraylab-2'\n",
    "\n",
    "# Enter you Access and Secret keys. They are the ones that were displayed in the instructions.\n",
    "aws_access_key_id = 'MEDG1N4RTERI81O9DJ5M'\n",
    "aws_secret_access_key = 'sR0eNR5XydTD5trF0U6ivktCACBkP00R0Oek4PY1'\n",
    "\n",
    "# Do not change this value, this is the internal location for the RGW\n",
    "endpoint_url = 'http://rook-ceph-rgw-ocs-storagecluster-cephobjectstore.openshift-storage.svc.cluster.local'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "Of course we'll need some libraries to work with, so import them by running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: boto3 in /opt/app-root/lib/python3.8/site-packages (1.17.11)\n",
      "Requirement already satisfied: botocore<1.21.0,>=1.20.11 in /opt/app-root/lib/python3.8/site-packages (from boto3) (1.20.95)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/app-root/lib/python3.8/site-packages (from boto3) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /opt/app-root/lib/python3.8/site-packages (from boto3) (0.3.7)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/app-root/lib/python3.8/site-packages (from botocore<1.21.0,>=1.20.11->boto3) (2.8.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/app-root/lib/python3.8/site-packages (from botocore<1.21.0,>=1.20.11->boto3) (1.26.5)\n",
      "Requirement already satisfied: six>=1.5 in /opt/app-root/lib/python3.8/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.21.0,>=1.20.11->boto3) (1.16.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.1.2; however, version 21.2.4 is available.\n",
      "You should consider upgrading via the '/opt/app-root/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install boto3\n",
    "import boto3\n",
    "import json\n",
    "import botocore\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S3 and SNS connections\n",
    "Boto3 is a standard library to interact with cloud services like S3 and SNS. As Ceph is compatible with S3 and SNS, we can directly use the library to work with the storage. First, let's create the clients to connect to the storage (you can see we are using some parameters we defined earlier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3',\n",
    "                endpoint_url = endpoint_url,\n",
    "                aws_access_key_id = aws_access_key_id,\n",
    "                aws_secret_access_key = aws_secret_access_key,\n",
    "                region_name = 'default',\n",
    "                config=botocore.client.Config(signature_version = 's3'))\n",
    "\n",
    "sns = boto3.client('sns', \n",
    "                endpoint_url = endpoint_url, \n",
    "                aws_access_key_id = aws_access_key_id,\n",
    "                aws_secret_access_key= aws_secret_access_key,\n",
    "                region_name='default', \n",
    "                config=botocore.client.Config(signature_version = 's3'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create buckets\n",
    "Now that we can connect to the storage, we can create our buckets. Run the first cell, which will define a \"creation function\" (an S3 API call using the client we created). Then the second cell that will create the 3 buckets we will need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bucket(bucket_name):\n",
    "    result = s3.create_bucket(Bucket=bucket_name)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'tx000000000000000000967-00614a1f76-14e8-ocs-storagecluster-cephobjectstore',\n",
       "  'HostId': '',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-request-id': 'tx000000000000000000967-00614a1f76-14e8-ocs-storagecluster-cephobjectstore',\n",
       "   'content-length': '0',\n",
       "   'date': 'Tue, 21 Sep 2021 18:07:50 GMT',\n",
       "   'connection': 'Keep-Alive'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_bucket(bucket_base_name)\n",
    "create_bucket(bucket_base_name+'-processed')\n",
    "create_bucket(bucket_base_name+'-anonymized')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verification\n",
    "As the previous output may have been cryptic (and anyway it's always good to check), let's list all the buckets and verify they indeed have been created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xraylab-2\n",
      "xraylab-2-anonymized\n",
      "xraylab-2-processed\n"
     ]
    }
   ],
   "source": [
    "for bucket in s3.list_buckets()['Buckets']:\n",
    "    print(bucket['Name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make buckets public read\n",
    "Our Grafana dashboard will display the last image from each bucket. Instead of setting up a dedicated web server, we can directly query our object stores to retrieve the images. For this to work we have to make our bucket \"public-readable\". This is done by applying to each this bucket policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for bucket in s3.list_buckets()['Buckets']:\n",
    "    bucket_policy = {\n",
    "                      \"Version\":\"2012-10-17\",\n",
    "                      \"Statement\":[\n",
    "                        {\n",
    "                          \"Sid\":\"AddPerm\",\n",
    "                          \"Effect\":\"Allow\",\n",
    "                          \"Principal\": \"*\",\n",
    "                          \"Action\":[\"s3:GetObject\"],\n",
    "                          \"Resource\":[\"arn:aws:s3:::{0}/*\".format(bucket['Name'])]\n",
    "                        }\n",
    "                      ]\n",
    "                    }\n",
    "    bucket_policy = json.dumps(bucket_policy)\n",
    "    s3.put_bucket_policy(Bucket=bucket['Name'], Policy=bucket_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bucket Notifications configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, let's define our endpoint (where we will send our notifications) through a small array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = {}\n",
    "attributes['push-endpoint'] = 'kafka://my-cluster-kafka-bootstrap.'+namespace+':9092'\n",
    "attributes['kafka-ack-level'] = 'broker'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we define a function that will create a topic with those attributes (I know we will create only one topic, so a function may seem too much, but now you have a reusable snippet for when you have lots to create)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_topic(topic):\n",
    "    topic_arn = sns.create_topic(Name=topic, Attributes=attributes)['TopicArn']\n",
    "    return topic_arn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the notification topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arn:aws:sns:ocs-storagecluster-cephobjectstore::xray-images'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_topic('xray-images')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And as always, a quick check that it has been created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Topics': [{'TopicArn': 'arn:aws:sns:ocs-storagecluster-cephobjectstore::xray-images'}],\n",
       " 'ResponseMetadata': {'RequestId': '8e461745-a03f-442c-a0af-16c7a0fa34b1.5352.2414',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-request-id': 'tx00000000000000000096e-00614a1f7b-14e8-ocs-storagecluster-cephobjectstore',\n",
       "   'content-type': 'application/xml',\n",
       "   'content-length': '977',\n",
       "   'date': 'Tue, 21 Sep 2021 18:07:55 GMT',\n",
       "   'connection': 'Keep-Alive'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sns.list_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next step is to define a notification configuration, i.e. when our topic should be used. Here it's whenever a new object is being created (\"Events\": \\[\"s3:ObjectCreated:*\"\\]), in which case we use our topic, refering to it through its ARN (unique id, 'arn:aws:sns:s3a::xray-images'). And we apply this configuration to our base bucket, the one where the images will arrive: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'tx00000000000000000096f-00614a1f7d-14e8-ocs-storagecluster-cephobjectstore',\n",
       "  'HostId': '',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-request-id': 'tx00000000000000000096f-00614a1f7d-14e8-ocs-storagecluster-cephobjectstore',\n",
       "   'content-length': '0',\n",
       "   'date': 'Tue, 21 Sep 2021 18:07:57 GMT',\n",
       "   'connection': 'Keep-Alive'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bucket_notifications_configuration = {\n",
    "            \"TopicConfigurations\": [\n",
    "                {\n",
    "                    \"Id\": 'xray-images',\n",
    "                    \"TopicArn\": 'arn:aws:sns:s3a::xray-images',\n",
    "                    \"Events\": [\"s3:ObjectCreated:*\"]\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "\n",
    "s3.put_bucket_notification_configuration(Bucket = bucket_base_name,\n",
    "        NotificationConfiguration=bucket_notifications_configuration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Last quick verfication that the configuration has been applied to our bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'tx000000000000000000970-00614a1f7f-14e8-ocs-storagecluster-cephobjectstore',\n",
       "  'HostId': '',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-request-id': 'tx000000000000000000970-00614a1f7f-14e8-ocs-storagecluster-cephobjectstore',\n",
       "   'content-type': 'application/xml',\n",
       "   'content-length': '223',\n",
       "   'date': 'Tue, 21 Sep 2021 18:07:59 GMT',\n",
       "   'connection': 'Keep-Alive'},\n",
       "  'RetryAttempts': 0},\n",
       " 'TopicConfigurations': [{'Id': 'xray-images',\n",
       "   'TopicArn': 'arn:aws:sns:ocs-storagecluster-cephobjectstore::xray-images',\n",
       "   'Events': ['s3:ObjectCreated:*']}]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3.get_bucket_notification_configuration(Bucket = bucket_base_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# You're done!\n",
    "Buckets have been created, notifications have been configured. You're now ready to run the demo. You can leave the notebook opened or close the tab, and go back to the Bookbag for the instructions on how to run the demo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
