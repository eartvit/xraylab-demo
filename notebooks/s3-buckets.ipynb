{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5d17096-844a-4c1b-9255-ba6186720e82",
   "metadata": {},
   "source": [
    "# Base S3 Bucket preparation and training data download to Jupyter Notebook Pod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3736fe2-3517-44f9-9635-1ddfb34c0594",
   "metadata": {},
   "source": [
    "#### This Notebook sets the scene for the base bucket and then downloads available x-ray images for training to the pod for the execution of the ML training notebook.\n",
    "\n",
    "#### Note! The actual upload of train/test/validation files is outside the scope of this notebook. Plese use aws cli to upload the dataset to the bucket.\n",
    "\n",
    "#### Important: This demo uses Rados GW and the below function is provided as guidance if one wishes to adapt the demo to use pure AWS S3 service instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a38ce02-ee0d-400a-ad4f-c7db350f4c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import botocore\n",
    "import json\n",
    "import os\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0b0178-4ea8-44e9-8271-d81df43b7b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# direct keys to S3 and not to the ODF storage instance from OCP4\n",
    "aws_access_key_id = '3XWIR321K1ERLV4FXK5D'\n",
    "aws_secret_access_key = 'Fg9c9MRoKzZ41sad7xGurblqmYe0XaKtrD2ZL0ve'\n",
    "region_name = 'default' #default region for the profile e.g., us-east-2\n",
    "\n",
    "# To reduce external traffic one can use the internal cluster service endpoint - it should look something like below and can be obtained from the openshift-storage namespace\n",
    "# endpoint_url = 'http://rook-ceph-rgw-ocs-storagecluster-cephobjectstore.openshift-storage.svc.cluster.local'\n",
    "\n",
    "# The external rados GW endpoint can be obtained from the networking-routes section of the administrator view having openshift-storage selected as project.\n",
    "endpoint_url = 'https://rgw-openshift-storage.apps.cluster-lv628.lv628.sandbox1664.opentlc.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9f8bef-7862-4986-a3f0-7b72c8e19b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bucket_aws(bucket_name):\n",
    "    location = {'LocationConstraint': region_name}\n",
    "    result = s3.create_bucket(Bucket=bucket_name, CreateBucketConfiguration=location)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a1b260-e442-4286-bcb9-b1a13f945d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bucket_rados(bucket_name):\n",
    "    result = s3.create_bucket(Bucket=bucket_name)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9872c042-1915-46bc-a6e1-f837d867bee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 AWS client - if not using Rados GW. This demo uses Rados GW and the below function is provided as guidance if one wishes to adapt the demo to use pure AWS S3 service instead.\n",
    "s3_aws = boto3.client('s3',\n",
    "                  aws_access_key_id = aws_access_key_id,\n",
    "                  aws_secret_access_key = aws_secret_access_key,\n",
    "                  region_name = region_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abd565b-5189-4366-ac72-e8da8f78d93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rados GW S3 compatible client\n",
    "s3 = boto3.client('s3',\n",
    "                endpoint_url = endpoint_url,\n",
    "                aws_access_key_id = aws_access_key_id,\n",
    "                aws_secret_access_key = aws_secret_access_key,\n",
    "                region_name = 'default',\n",
    "                config=botocore.client.Config(signature_version = 's3'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1279b7a3-a46d-434c-872f-77fcc1d0c134",
   "metadata": {},
   "source": [
    "### Create required buckets and set permissions. \n",
    "Optionally, you can change the name of the bucket, though ensure to replace the new name in all instances where you use it (this file included)\n",
    "\n",
    "#### Note: run the below cell only once!\n",
    "\n",
    "bucket_base_name: the bucket used by the image uploader utility to drop new x-ray images. This is the base bucket used as input by the ML prediction service.</br>\n",
    "bucket_base_name + '-train-test-valid' : the bucket used for training a new model, if so desired. The project has an example trained model ready for use.</br>\n",
    "bucket_base_name + '-datasource': the bucket where the image uploader utility will take new images to simulate an upload.</br>\n",
    "bucket_base_name + '-processed': the bucket where the ML prediction service will move evaluated x-ray images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c4efcc-f3d4-4908-9bca-736cb085507d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_base_name = 'ml-pneumonia'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c854c911-927f-47ab-b261-5ba4dea7eafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_bucket_rados(bucket_base_name)\n",
    "create_bucket_rados(bucket_base_name + '-train-test-valid')\n",
    "create_bucket_rados(bucket_base_name + '-datasource')\n",
    "create_bucket_rados(bucket_base_name + '-processed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c6425e-d4ee-434d-8f40-1511e38e7ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = s3.list_buckets()\n",
    "\n",
    "# Output the bucket names\n",
    "print('Existing buckets:')\n",
    "for bucket in response['Buckets']:\n",
    "    print(f'  {bucket[\"Name\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12102de-cf69-4d77-a81f-c4ff0f1116f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for bucket in s3.list_buckets()['Buckets']:\n",
    "    bucket_policy = {\n",
    "                      \"Version\":\"2012-10-17\",\n",
    "                      \"Statement\":[\n",
    "                        {\n",
    "                          \"Sid\":\"AddPerm\",\n",
    "                          \"Effect\":\"Allow\",\n",
    "                          \"Principal\": \"*\",\n",
    "                          \"Action\":[\"s3:GetObject\"],\n",
    "                          \"Resource\":[\"arn:aws:s3:::{0}/*\".format(bucket['Name'])]\n",
    "                        }\n",
    "                      ]\n",
    "                    }\n",
    "    bucket_policy = json.dumps(bucket_policy)\n",
    "    s3.put_bucket_policy(Bucket=bucket['Name'], Policy=bucket_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afad0e9c-dfdb-43dc-9479-b8ac19275a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for bucket in s3.list_buckets()['Buckets']:\n",
    "    print(bucket['Name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6915a7cd-e9b5-4ae4-8949-ddafcb4d5cdc",
   "metadata": {},
   "source": [
    "### Section for downloading files to the Juphyter Pod. Use only if you want to retrain the ML algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2dc2e0-1c8b-4d8b-92ad-477f57883036",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def download_dir_rados(aws_access_key_id, aws_secret_access_key, region_name,  bucket, s3_prefix = '', local_base = ''):\n",
    "    \"\"\"\n",
    "    params:\n",
    "    - aws_access_key_id: The aws_access_key_id\n",
    "    - aws_secret_access_key: The aws_secret_access_key\n",
    "    - region_name: The region where the bucket was created\n",
    "    - bucket: s3 bucket with target contents\n",
    "    - s3_prefix: pattern to match in s3\n",
    "    - local_base: local path to folder in which to place files\n",
    "    \"\"\"\n",
    "    \n",
    "    s3_resource = boto3.resource(service_name= 's3',\n",
    "                                 endpoint_url = endpoint_url,\n",
    "                                 aws_access_key_id = aws_access_key_id,\n",
    "                                 aws_secret_access_key = aws_secret_access_key,\n",
    "                                 region_name = region_name,\n",
    "                                 config=botocore.client.Config(signature_version = 's3')\n",
    "                                )\n",
    "    \n",
    "    ml_ds_bucket = s3_resource.Bucket(bucket)\n",
    "    bucket_objects = ml_ds_bucket.objects.all()\n",
    "    \n",
    "    files = []\n",
    "    for item in bucket_objects:\n",
    "        files.append(item.key)\n",
    "\n",
    "    print(f'Downloading files...')\n",
    "    for file in tqdm.tqdm(files):\n",
    "        dest_pathname = os.path.join(local_base, file)\n",
    "        if not os.path.exists(os.path.dirname(dest_pathname)):\n",
    "            os.makedirs(os.path.dirname(dest_pathname))\n",
    "        ml_ds_bucket.download_file(file, dest_pathname)\n",
    "        \n",
    "    print(f'Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f65d76-6e88-49a0-98e1-9f258d835174",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def download_dir_aws(aws_access_key_id, aws_secret_access_key, region_name,  bucket, s3_prefix = '', local_base = ''):\n",
    "    \"\"\"\n",
    "    params:\n",
    "    - aws_access_key_id: The aws_access_key_id\n",
    "    - aws_secret_access_key: The aws_secret_access_key\n",
    "    - region_name: The region where the bucket was created\n",
    "    - bucket: s3 bucket with target contents\n",
    "    - s3_prefix: pattern to match in s3\n",
    "    - local_base: local path to folder in which to place files\n",
    "    \"\"\"\n",
    "    \n",
    "    s3_resource = boto3.resource('s3',\n",
    "                             aws_access_key_id = aws_access_key_id,\n",
    "                             aws_secret_access_key = aws_secret_access_key,\n",
    "                             region_name = region_name)\n",
    "    \n",
    "    ml_ds_bucket = s3_resource.Bucket(bucket)\n",
    "    bucket_objects = ml_ds_bucket.objects.all()\n",
    "    \n",
    "    files = []\n",
    "    for item in bucket_objects:\n",
    "        files.append(item.key)\n",
    "\n",
    "    print(f'Downloading files...')\n",
    "    for file in tqdm.tqdm(files):\n",
    "        dest_pathname = os.path.join(local_base, file)\n",
    "        if not os.path.exists(os.path.dirname(dest_pathname)):\n",
    "            os.makedirs(os.path.dirname(dest_pathname))\n",
    "        ml_ds_bucket.download_file(file, dest_pathname)\n",
    "        \n",
    "    print(f'Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4041a6f4-2cb8-4aca-a850-a3e83e197f9a",
   "metadata": {},
   "source": [
    "#### The below cell will download to this pod in the (new) dataset folder the contents of the S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43f6caa-759b-47e8-8f27-18d76b46e588",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_dir_rados(aws_access_key_id = aws_access_key_id,\n",
    "              aws_secret_access_key = aws_secret_access_key,\n",
    "              region_name = region_name,\n",
    "              bucket = 'ml-pneumonia--train-test-valid,\n",
    "              s3_prefix = '',\n",
    "              local_base = 'dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b05a27-0ba6-4231-a6bf-3bcf4c15c927",
   "metadata": {},
   "source": [
    "Final check to ensure the number of files matches the one from the bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70cd17b-c64e-4f91-a723-546442e8c4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lR dataset | wc -l"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
